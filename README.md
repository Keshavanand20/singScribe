SignScribe leverages advanced technologies to interpret sign language gestures in real time and translate them into multiple languages, enhancing communication for impaired individuals.

Tech Stacks Used:
1. Computer Vision: Implemented YOLO (You Only Look Once) for efficient and accurate hand detection.
2. Machine Learning: Trained TensorFlow models, including MobileNet, for precise gesture recognition and classification.
3. Natural Language Processing: Utilized Google's Gemini API and PaLM (Phrase-augmented Language Model) to enhance the contextual understanding and translation accuracy of detected sign language gestures.
4. Front-End: Developed using Flask to provide a user-friendly interface for seamless interaction.
5. Programming Languages: Primarily used Python for developing and integrating the models.
6. Frameworks and Libraries: OpenCV for image processing and TensorFlow for model training and deployment.
